// voice.js
console.log("ðŸŽ™ Voice module loaded");

// ðŸŽ§ ÐžÐ·Ð²ÑƒÑ‡Ð¸Ð²Ð°Ð½Ð¸Ðµ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð°
function speakOutput() {
  const output = document.getElementById("output");
  if (!output || !output.textContent.trim()) return;
  const utterance = new SpeechSynthesisUtterance(output.textContent);
  const lang = document.getElementById("targetLang")?.value || "en";
  utterance.lang = lang;
  speechSynthesis.speak(utterance);
}

// ðŸŽ¤ Ð“Ð¾Ð»Ð¾ÑÐ¾Ð²Ð¾Ð¹ Ð²Ð²Ð¾Ð´ (Speech-to-Text)
const micButton = document.getElementById("mic-button");
const inputField = document.getElementById("inputText");

if (micButton && inputField) {
  let recognition;
  let isListening = false;

  const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

  if (!SpeechRecognition) {
    micButton.disabled = true;
    micButton.title = "ðŸŽ¤ Ð‘Ñ€Ð°ÑƒÐ·ÐµÑ€ Ð½Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ Ð³Ð¾Ð»Ð¾ÑÐ¾Ð²Ð¾Ð¹ Ð²Ð²Ð¾Ð´";
  } else {
    recognition = new SpeechRecognition();
    recognition.continuous = false;
    recognition.interimResults = false;

    recognition.onstart = () => {
      isListening = true;
      micButton.textContent = "ðŸ›‘";
    };

    recognition.onresult = (event) => {
      const transcript = event.results[0][0].transcript;
      inputField.value += (inputField.value ? " " : "") + transcript;
      micButton.textContent = "ðŸŽ¤";
      isListening = false;
    };

    recognition.onerror = (e) => {
      console.error("ðŸŽ™ ÐžÑˆÐ¸Ð±ÐºÐ° Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ:", e.error);
      micButton.textContent = "ðŸŽ¤";
      isListening = false;
    };

    recognition.onend = () => {
      micButton.textContent = "ðŸŽ¤";
      isListening = false;
    };

    micButton.addEventListener("click", () => {
      if (isListening) {
        recognition.stop();
      } else {
        recognition.lang = document.getElementById("sourceLang")?.value || "auto";
        recognition.start();
      }
    });
  }
} else {
  console.warn("ðŸŽ¤ Mic button Ð¸Ð»Ð¸ inputText Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹ Ð² DOM.");
}