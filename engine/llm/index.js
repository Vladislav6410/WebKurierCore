// WebKurierCore/engine/llm/index.js
import { secretManager } from './secrets/manager.js';
import { validateConfig } from './config.js';

/**
 * Secure LLM Integration Layer
 * Handles communication with language models through secure channels
 */

class LLMClient {
  constructor(config = {}) {
    this.config = {
      defaultModel: "qwen3-coder",
      timeout: 30000,
      retryAttempts: 3,
      ...config
    };
    
    console.log("üîê Secure LLM Client initialized");
  }

  /**
   * Initialize LLM client with security validation
   * @returns {Promise<boolean>} Initialization success
   */
  async initialize() {
    if (!(await secretManager.initialize())) {
      throw new Error("Failed to initialize secret manager");
    }
    
    if (!validateConfig()) {
      throw new Error("Configuration validation failed");
    }
    
    console.log("‚úÖ LLM Client fully initialized with security validation");
    return true;
  }

  /**
   * Get provider-specific configuration with secrets
   * @param {string} provider - Provider name
   * @returns {Object} Provider config with secrets
   */
  getProviderConfig(provider) {
    const secrets = secretManager.getProviderSecrets(provider);
    
    if (!secrets.apiKey) {
      throw new Error(`No API key available for provider: ${provider}`);
    }
    
    switch (provider.toLowerCase()) {
      case 'qwen':
        return {
          apiKey: secrets.apiKey,
          baseUrl: secrets.baseUrl,
          headers: {
            'Authorization': `Bearer ${secrets.apiKey}`,
            'Content-Type': 'application/json',
            'User-Agent': 'WebKurier/2.0.0'
          }
        };
      case 'openai':
        return {
          apiKey: secrets.apiKey,
          baseUrl: secrets.baseUrl,
          headers: {
            'Authorization': `Bearer ${secrets.apiKey}`,
            'Content-Type': 'application/json',
            'OpenAI-Beta': 'assistants=v2'
          }
        };
      default:
        throw new Error(`Unsupported provider: ${provider}`);
    }
  }

  /**
   * Chat with LLM through secure channel
   * @param {Object} options - Chat options
   * @param {Array} options.messages - Conversation messages
   * @param {string} options.agent - Agent type
   * @param {string} options.task - Task type
   * @param {number} [options.temperature=0.1] - Temperature
   * @param {string} [options.model] - Model override
   * @returns {Promise<Object>} Response object
   */
  async chat(options) {
    const {
      messages,
      agent,
      task,
      temperature = 0.1,
      model = this.config.defaultModel
    } = options;

    // Determine provider based on model
    const provider = model.includes('qwen') ? 'qwen' : 'openai';
    const providerConfig = this.getProviderConfig(provider);

    console.log(`ü§ñ Secure LLM Request: agent=${agent}, task=${task}, model=${model}, provider=${provider}`);
    
    try {
      // In production, this would make actual API calls with proper error handling
      const requestBody = {
        model: model,
        messages: messages,
        temperature: temperature,
        stream: false
      };

      // Mock response for demonstration - replace with actual API call
      const mockResponses = {
        'engineer': {
          'default': `// Generated code for: ${messages[messages.length-1].content}\nconsole.log('Securely generated by Engineer Agent');`,
          'fast': `// Quick patch for: ${messages[messages.length-1].content}\n// Applied securely`,
          'architecture': `{\n  "project": "secure-generated-structure",\n  "folders": ["src", "tests", "docs"],\n  "files": ["index.js", "package.json"],\n  "security": "enabled"\n}`,
          'code-generation': `function secureGeneratedFunction() {\n  // Production-ready implementation\n  return 'securely-generated';\n}`
        }
      };

      const responseText = mockResponses[agent]?.[task] || `Secure response for ${agent}/${task}`;
      
      return {
        text: responseText,
        model: model,
        tokens: responseText.length,
        finish_reason: 'stop',
        provider: provider
      };
    } catch (error) {
      console.error("‚ùå Secure LLM Error:", error.message);
      throw error;
    }
  }
}

export const llm = new LLMClient();
